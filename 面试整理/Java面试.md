# 1.多线程高并发

## 1.1.进程、线程、协程

1. 程序：存放在磁盘中的可执行的应用软件。

2. 进程：
    1. 是资源分配的基本单位；
    2. 是程序启动后从磁盘进入到被分配的内存的资源和代码的集合；
    3. 也是CPU指令和内存数据的集合。
    
3. 线程：
    1. 程序执行的基本单位；
    2. 进程中代码执行的路径；
    3. 从main线程这条路径开始执行。
    
4. 进程和线程的区别：

    1. 以上面概念的角度解释；
    2. 以JVM的角度解释：一个jvm进程运行时所管理的内存区域如下图，一个进程中可以存在多个线程，多个线程共享堆空间和本地方法区（元空间），每个线程有自己的虚拟机栈、本地方法栈和程序计数器。![image-20200928231644475](assets/image-20200928231644475.png)

5. 进程的状态：

    ![image-20200928232501687](assets/image-20200928232501687.png)

    1. 创建状态（new）：进程正在被创建，尚未就绪；
    2. 就绪状态（ready）：进程处于准备运行阶段，即进程获得了除cpu以外的所有资源；
    3. 运行状态（running）：进程正在cpu上运行（cpu的任意核心的任意时刻只有一个进程处于运行中）；
    4. 阻塞状态（waiting）：又称等待状态，进程正在等待某一事件而暂停运行（如：等待某资源可用，等待IO操作完成），这时即使cpu空闲，该进程也不能运行；
    5. 结束状态（terminated）：进程正在从系统中消失，可能是进程正常结束或其他原因退出运行。

6. 进程间的通信方式：

    1. 管道/匿名管道（Pipes）：用于具有亲缘关系的父子进程或兄弟进程间的通信，存在于内存中；
    2. 命名管道（Names Pipes）：匿名管道由于没有名字，只能用于亲缘进程之间通信。命名管道的提出就是解决这个问题的，该种管理严格遵循西先进先出（FIFO），以磁盘文件的的方式存在，可以实现本机任意两个进程间的通信；
    3. 信号（Signal）：用于通知接收进程某个事件已经发生；
    4. 消息队列（Message Queuing）：是存放在内存中具有特定格式的消息链表，和管道一样是FIFO的规则。其和内核绑定，只有在内核重启或显示的被删除就会真正的删除。消息队列可以实现消息的随机查询，不一定要FIFO的读取，可以按照消息类型读取。MQ克服了信号承载信息少，管道只能承载无格式字节流以及缓冲区大小受限问题；
    5. 信号量（Semaphores）：是一个计数器，用于多线程对共享数据的访问，信号量的意图是进程间同步，主要用于解决与同步相关的问题并避免竞争条件；
    6. 共享内存（Shared memory）：使多个进程可以访问同一块内存，不同进程可以及时看到对方进程对共享内存数据的修改。这种方式需要依靠某种同步操作，如互斥锁和信号量等；
    7. 套接字（Sockets）：该方法主要用于客户端和服务器间通过网络进行通信。套接字是支持TCP/IP网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，即通信双方的一种约定，用套接字中提供的函数来完成通信过程。

7. 线程间的同步方式：

    1. 互斥量（Mutex）：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所有可以保证公共资源不会被多个线程同时访问，如java中的synchronized和lock锁；
    2. 信号量（Semphares）：允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量；
    3. 事件（Event）：Wait/Notify操作，通过通知操作的方式来保持多线程同步，还可以方便的实现多线程的优先级。

8. 进程的调度算法：

    1. 先到先服务调度算法（FCFS）：从就绪队列中出队一个进程为之分配cpu资源，使其立即执行直到执行完成或发生某事件而被阻塞放弃cpu的占用再重新调度；
    2. 短作业优先调度算法（SJF）：从就绪队列中选出一个估计运行时间最短的进程为之分配资源，使它立即执行直到执行完成或发生某事件被阻塞放弃cpu的占用再重新调度；
    3. 时间片轮转调度算法（Round robin）：每个进程都会被分配一个时间段（时间片），即该进程允许被cpu运行的时间；
    4. 多级反馈队列调度算法：既能使高优先级进程得到响应，又能短作业进程迅速完成（unix使用）；
    5. 优先级调度：为每个进程分配优先级，按照优先级依次执行，具有相同优先级的按FCFS方式执行，优先级的确认可以通过内存要求，时间要求或其他资源要求来确认。

9. 协程（纤程）：

    1. 线程中的多条执行路径；
    2. jvm的线程和os内核线程是1:1的关系，每启动一个线程就需要和os交互，开销大。而协程就是在用户态模拟内核级别线程的调度，在用户态的内存空间维护寄存器信息和堆信息等，这样用户级别的线程（即协程）无需和os交互就能撑起并发执行；
    3. 应用场景：用户空间的异步编程，用于回调函数。

10. 程序的本质：
    1. 程序的本质就是CPU可以执行的汇编指令和内存中的数据；
    2. 从内存中读出PC（指令计数器）当前指向的指令地址和对应数据，通过总线写入CPU的寄存器中；
    3. CPU的ALU（逻辑计算单元）会进行计算，并将计算结果写回到内存中；
    4. 此时CPU的PC会指向下一条指令；
    5. 当CPU的核心切换到其他线程执行时，当前中断的线程相关的数据（寄存器数据，堆栈信息）会被暂存在内存中，等下次切换回来时从中断的位置继续执行。

11. 线程切换的概念是什么：当CPU发生线程切换时，前者的执行状态会存入CPU Cache中，用于下次切换回来继续执行。
      1. 问：多线程可以提高效率，那是不是线程越多越好？
      2. 答：不是，因为更多的线程就意味着CPU要在切换下上文这个操作上面消耗更多的开销。

12. 什么是用户级别线程？什么是内核级别线程？
      1. 从java的角度来看，jvm的用户线程和操作系统的内核线程是1:1的关系；
      2. 从golang的角度来看，用户线程和内核线程是M:N的关系，而且M远远大于N；

13. Golang的GPM：
       1. 自动创建一个线程池，维护一批内核线程，go关键字会将指定的任务存入任务队列中，由预先创建好的内核线程执行；
       2. 比起java，golang可以用更小的上下文切换的开销换取更大量的任务并发执行，golang的任务就相当于用户线程；
       3. 类似于java的线程池的概念，ForkJoinPool线程池，区别在于java线程池中的任务无法同步通信，而golang可以通过channel来进行任务间的同步和通信。

14. **有没有遇到过OOM的场景？**重写了Object类的finalize()方法，该方法能自定义对象回收策略；  不断有新的对象涌入堆内存，重写的对象回收机制相当耗时，很快内存就报OOM。

## 1.2.volatile、synchronized、无锁、偏向锁、轻量级锁、重量级锁、锁升级、CAS、AQS

1. 什么是轻量级锁？什么是重量级锁？
    1. 轻量级锁又称自旋锁，需要获取被锁定的锁的线程都需要自旋等待锁的释放；
    2. 重量级锁，需要获取被锁定的锁的线程需要进入等待队列中排队，等待操作系统调度。
    3. 问：轻量级锁一定比重量级锁轻吗？或者说一定效率更高吗？不一定，如果锁的竞争非常激烈，有非常多的线程在自旋等待锁，则CPU的资源会大量消耗在Context Switch上面（即不断切换线程去执行循环操作）。



# 2.从OS原理、CPU原理、Linux内核、计算机组成原理到JVM原理

* 操作系统及其内核：
  1. **什么是操作系统？**是一种运行在硬件系统上的特殊的软件程序，既能管理计算机的硬件和软件资源，又能为用户提供与系统交互的界面，内核就是操作系统的核心逻辑；
  2. **什么是内核？**以unix/linux系统为主，负责管理文件系统、应用进程调度、中断处理设备驱动、cpu调度、内存、文件系统、网络系统等，是连接应用程序和硬件的桥梁；
  3. 宏内核：kernel和其周边被其管理的如cpu调度、文件系统、内存管理等功能划分为一个整体，将这个整体当作操作系统的核心，称为宏内核；
  4. 微内核：kernel内核只负责进程调度，而其他如cpu调度、文件系统、内存管理等功能都可能是以分布式形式存在的（不同的核心管理不同的功能），所有功能之间的交互都需要通过kernel内核进行调度，如：用户访问文件系统，需要通过kernel代理；文件系统和cpu调度交互，也需要kernel进行代理。
  5. 外内核：会根据当前运行的应用自动调整使其更适合应用程序运行；
  6. 虚拟化：通过底层的虚拟化技术管理多个虚拟的os以充分的利用硬件资源。
  
* 操作系统启动原理：
  1. 开机 -> 首先给主板通电 -> 主板上有一块BIOS芯片会加点自检，检测硬件的故障问题，自检完毕后加载bootloader到内存 -> 由bootloader启动操作系统（从硬盘到内存），在此之前的操作系统存储在磁盘MBR中，即磁盘的第一个扇区 -> os启动后开始接管硬件系统。
  2. 在os未启动之前，有些针对计算机硬件的设置信息，如：启动硬盘还是软盘等，会被写入到主板上的另一块芯片cmos中，这块芯片由电池供电。
  
* 操作系统的中断：
  1. 硬件中断信号：硬件通过发送中断信号和操作系统产生实时的交互。如键盘鼠标等设备被触发时会给os发送一个中断信号，os会中断目前正在处理的任务，根据该中断信号去os内部的中断异常处理表中查询对应的号别，根据号别做出不同的处理；
  2. 软中断：应用程序与操作系统的中断信号只有一个，也就是0x80号中断。
  
* 操作系统的系统调用：
  * **什么是系统调用？**先说明用户态和内核态的概念。
  * 内核态和用户态：
    * CPU指令级别：intel的cpu将指令级别划分为0、1、2、3四个级别，用于区分不同级别和优先级的指令操作；
    * 其中os发出的都是0级指令，用户发出的都是3级指令，通过指令级别的划分，将os操作的内核态内存和用户态内存区分，即用户级别的指令操作无法访问os的内核态内存，提高了os的安全性；
    * 用户态（user mode）和内核态（kernel mode）是根据访问资源的特点，把进程在系统上的运行分为两个级别；
    * 处于用户态的进程只能操作用户程序相关的数据，处于内核态的进程能够操作计算机的任何资源。
  * 系统调用：在运行用户程序的过程中，凡是与内核态级别的资源有关的操作（如：文件关联、进程控制、内存管理），都必须通过系统调用的方式向os内核提出服务请求，并由os代为完成。
  * 系统调用按功能分类：
    * 设备管理，完成设备的请求或释放，以及设备启动等功能；
    * 文件管理，完成文件的读、写、创建及删除等功能；
    * 进程控制，完成进程的创建、撤销、阻塞及唤醒等功能；
    * 进程通信，完成进程之间的消息传递或信号传递等功能；
    * 内存管理，完成内存的分配、回收以及获取作业占用内存区大小及地址等功能。
  * 应用程序和内核态进行交互（即系统调用内核态的函数）：
    1. 应用程序发出0x80中断指令（同时发送需要调用的函数的编号和参数）或调用sysenter原语（汇编层面的原语，并非所有CPU都支持）；
    2. os进入内核态；
    3. 根据应用程序发来的编号在中断向量表中查找处理例程（即对应的内核态系统函数）；
    4. 保存硬件现场（cs，ip等寄存器值）；
    5. 保存应用程序现场（堆栈与寄存器值）；
    6. 执行中断例程system_call：
       1. 根据参数与编号寻找对应例程；
       2. 执行并返回。
    7. 恢复现场；
    8. os返回用户态；
    9. 应用程序继续执行。
  
* 计算机的组成：
  * CPU：
    * PC：指令计数器，存储指令在内存中的地址，cpu会根据该地址从内存中将指令对应的数据读取到寄存器中），交由alu进行具体计算，本次计算完成在则指向下一条指令；
    * Registers：寄存器，cpu中存在多个寄存器，用于保存从内存中读取的数据；
    * ALU：逻辑计算单元，从寄存器中获取数据进行计算，并将结果写回内存；
    * MMU：内存管理单元，负责cpu的虚拟寻址，即将虚拟地址翻译成物理地址，然后才能访问真实的物理内存；
    * Cache：高速缓存，因为CPU和内存的速度相差巨大，所以在二者中间添加了三级高速缓存做为中间层。多核CPU的每个核心都有自己独立的一级二级缓存，共用一个三级缓存。
  * 总线：用于连接计算机各个组件并提供数据传输的一组导线。
  * ALU的超线程概念：单核cpu只有一组寄存器和指令计数器，每次切换线程都需要保存现场和恢复现场。为了提高效率，单核cpu划分多组寄存器和pc，每一组管理一个线程的信息，利用alu的高速在多组间不断切换计算以提高效率。
  
* 存储器层次结构：
  1. 远程文件存储 -> 磁盘 -> 主存 -> 三级缓存 -> 二级缓存 -> 一级缓存 -> CPU寄存器；
  3. 按块读取：即从内存中一次性读取一块存入缓存中，利用空间局部性原理（如一个数组，内存空间是紧挨着的），可以提高效率，充分发挥CPU一次性读取更多数据的能力；
  4. 缓存行：越大，局部性空间效率越高，读取时间越慢；越小，局部性空间效率越低，读取时间越块；目的工业实践的结果是64byte比较合适。
  5. MESI Cache一致性协议（Intel芯片上采用的缓存一致性协议）：
     1. Modified：已修改的，一块数据存在于两颗CPU的缓存行中，若其中一个被修改则置为该状态并通知其他CPU已修改；
     2. Exclusive：独占的；
     3. Shared：共享的；
     4. Invalid：失效的，收到已修改通知的CPU需要将对应的缓存行置换为已失效状态，重新去主存读取；
     5. 注：有些无法被缓存的数据，或跨越多个缓存行的数据，依然需要使用总线锁。
  
* 操作系统的内存管理：

  1. **操作系统的内存管理主要是做什么？**负责内存的分配和回收（malloc函数申请内存，free函数释放内存），另外地址转换也就是将逻辑地址转换成相应的物理地址等功能也是内存管理做的事。
  2. **内存管理有哪几种方式？**
     1. 连续分配管理：指为用户程序分配一段连续的内存空间，如：块式管理；
     2. 非连续分配管理：指为用户程序分配的内存空间是离散的不相邻的，如：页式，段式管理。
  3. **内存管理机制：**
     1. 块式管理：将内存分成几个固定大小的块，每个块只存储一个进程的数据。如果应用程序需要申请内存的话，os就分配一个内存块给它，不论应用程序需要的内存是大是小，统一分配一块，就会造成块中内存的浪费，这些块中未被利用的空间被称为碎片；
     2. 页式管理：把主存分为大小相等且固定的一页一页的形式，页较小，比块划分力度大，提供了内存利用率，减少碎片。页式管理通过页表对应逻辑地址和物理地址；
     3. 段式管理：把主存分为一段一段的，每一段的空间比页空间小很多并且不固定，段具有实际意义，即每个段对应了一组逻辑信息，如：主程序段MAIN、子程序段X、数据段D及栈段S等。段式管理通过段表对应逻辑地址和物理地址；
     4. 段页式管理：结合了段式和页式的优点，把主存分成若干段，每个段又分为若干页，即这种管理机制中段与段之间以及段的内部都是离散的。
  4. 快表和多级页表：
     1. 在分页内存管理中，最重要的是虚拟地址到物理地址的快速转换和虚拟地址空间大页表页会很大的问题；
     2. 快表：
        1. 为解决虚拟地址到物理地址的转换速度问题，os在页表方案上引入快表来加速。可以把快表理解成一种特殊的高速缓冲存储器（Cache），内容是页表的一部分或全部；
        2. 使用页表管理内存，在无快表的情况下，cpu读写内存数据时需要两次访问主存，一次访问页表获取物理地址，一次访问物理地址获取数据；
        3. 在有快表的情况下，cpu只需要访问一次高速缓存，一次主存即可。
     3. 多级页表：为了避免把全部页表一直放在内存中占用过多空间，而引入的节约内存的方案，属于用时间换空间的典型应用场景。
     4. 总结：为了提高内存空间的性能，提出了多级页表的概念，但是也引入了时间性能浪费的问题，因此提出了快表来补充损失的时间性能。
  5. 分页机制和分段机制的共同点和区别：
     1. 共同点：
        1. 分页机制和分段机制都是为了提高内存利用率，减少内存碎片；
        2. 页与页段与段之间是离散分配内存的，但页和段中的内存是连续的。
     2. 区别：
        1. 页的大小是固定的，由os决定；段的大小不固定，取决于当前运行的程序；
        2. 分页仅仅是为了满足os内存管理的需求，而段对应逻辑信息的单位，在程序中可以体现为代码段或数据段，能够更好的满足用户的需求。
  6. 逻辑地址和物理地址：
     1. 逻辑（虚拟）地址：程序设计语言和逻辑地址打交道，如：c中的指针存储的数值就是内存的逻辑地址，逻辑地址由os决定；
     2. 物理地址：指真实物理内存单元的地址。
  7. **CPU的寻址是什么？**cpu的寻址是指cpu通过其中的单元mmu翻译虚拟地址为物理地址，然后访问真实内存地址的过程。
  8. **为什么需要虚拟地址空间？**
     1. 若是没有虚拟地址，程序直接访问和操作物理内存存在的问题：
        1. 用户程序可以访问任意内存，寻址内存的每个字节，这种无限制的操作容易破坏os；
        2. 运行多个程序特别困难，两个应用程序同时对某段地址赋值，会产生数据冲突。
     2. 虚拟地址空间带来的优势有：
        1. 程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的大内存缓存区；
        2. 程序可以使用一系列的虚拟地址来访问大于可用物理内存的内存缓冲区。当物理内存的供应量变小时，内存管理器会将物理内存页（4kb）保存到磁盘文件。数据或代码页会根据需要在物理内存与磁盘间移动；
        3. 不同进程使用的虚拟地址彼此隔离，一个进程中的代码无法更改正在另一进程或操作系统使用的物理内存。

* 操作系统的虚拟内存：

  1. **什么是虚拟内存？**

     1. 虚拟内存可以让程序拥有超过系统物理内存大小的可用内存空间。同时也使得应用程序认为它拥有连续的可用的内存（一个连续完整的地址空间），而实际上，它通常是被分隔成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换；
     2. 为每个进程提供一个一致的、私有的地址空间，从而产生一种自己在独立主存的错觉，即每个进程拥有一片连续完整的内存空间以便更有效的管理内存；
     3. 虚拟内存的重要意义是定义了一个**连续的虚拟地址空间**（同一个程序使用的物理内存空间可能是不连续的，中间可能夹杂着其他进程的内存空间），并把内存扩展到硬盘空间。

  2. Linux中的虚拟内存系统：

     ![image-20200929172225277](assets/image-20200929172225277.png)

     * linux为每个进程维护一个单独的虚拟地址空间，该空间分为内核空间和用户空间，用户空间包含代码、数据、堆、共享库以及栈，内核空间包括内核中的代码和数据结构，内核空间中的某些区域被映射到所有进程共享的物理页面；
     * linux将一组连续的虚拟页面（大小等同于内存总量）映射到相应的一组连续的物理页面，这种做法为内核提供了一种便利的方法来访问物理内存中任何特定的位置。

  3. 局部性原理（高速缓存原理）：

     1. 局部性是虚拟内存技术的基础，程序运行正是具有局部性，才能只装入部分程序到内存就能运行；
     2. 局部性规律：就是说在某个较短的时间段内，程序执行局限于某一个小部分，访问的存储空间也局限于某个区域；
     3. **时间局部性**：如果程序中的某条指令一旦执行，不久后该指令可能会再次执行；如果某数据被访问过，不久后该数据可能被再次访问。产生时间局部性的原因是因为程序中存在大量的循环；
     4. **空间局部性**：一旦程序访问了某个存储单元，不久后其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这时因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表的形式簇聚存储的。
     5. 时间局部性是通过将最近使用的指令和数据保存到高速缓存中，并使用高速缓存的层次结构实现；
     6. 空间局部性通常使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现；
     7. 虚拟内存技术就是建立了“内存-外存”的两级存储器结构，利用局部性原理实现高速缓存，即连续的局部的内存空间，同样利用局部性原则的还有cpu高速缓存的缓存行概念；
     8. 局部性原则保证了在任意时刻，程序将趋向于在一个较小的活动页面集合上工作，这个集合被称为工作集。根据时间和空间局部性原则，只要将工作集缓存在物理内存中，接下来的地址翻译请求很大几率都在其中，从而减少了额外的硬盘流量。

  4. 虚拟存储器（外存+内存=虚拟存储器）：

     1. 基于局部性原理，在程序装入时，可用只装入一部分，其他部分留在外存，就可以启动程序执行，由于外存远大于内存，所以运行的软件内存大小可以大于计算机系统实际的内存大小；
     2. 在程序执行过程中，当所访问的信息不在内存时，有os将所需的部分调入内存，然后继续执行程序；
     3. 另外，os将内存中暂时不用的内容换到外存上，从而腾出空间存放将要调入内存的信息，这样计算机就好像为用户提供了一个比实际内存大得多得存储器，即虚拟存储器。

  5. 虚拟内存的技术实现：建立在离散分配的内存管理方式的基础上

     1. 请求分页存储管理：建立在分页管理之上，在作业开始运行前，仅装入当前要执行的部分分段即可运行，假如在作业运行过程中发现要访问的页面不在内存，则由处理器通知os按照对应的页面置换算法将相应的页面调入主存，同时os可以将暂时不用的页面置换到外存；
     2. 请求分段存储管理：建立在分段管理之上，增加了请求调段功能、分段置换功能。请求分段存储管理方式就如同请求分页存储管理方式一样；
     3. 请求段页式存储管理：同上；
     4. 请求分页存储管理和分页存储管理的区别：根本区别就是是否将程序所需的所有地址空间全部装入主存；
     5. 虚拟内存技术的实现一般要满足：
        1. 一定量的内存和外存：在载入程序时，只需要将程序的一部分装入内存，而将其他部分留在外存，直接执行程序；
        2. 缺页中断：如果需要执行的指令或访问的数据尚未在内存中，即缺页或缺段现象，则由cpu通知os将相应的页面或段调入内存，然后继续执行；
        3. 虚拟地址空间：逻辑地址到物理地址的变换。

  6. 页面置换算法：

     1. 地址映射过程中，若在发现所要访问的页面不在内存中，则发生缺页中断，需要os将其从外存调入内存，这时，被内存映射的文件就是一个分页交换文件；
     2. 当发生缺页中断时，如果当前内存没有空闲的页面可供调度，os就需要在内存中选择一个页面将其移出内存，为需要调入的页面腾出空间，用来选择淘汰哪一页的规则叫做页面置换算法；
     3. **OPT页面置换算法**（最佳页面置换）：该算法选择的页面是以后永不使用的，或者长时间不再访问的页面，这一可以保证获得最低的缺页率，但无法实现，仅作为参考；
     4. **FIFO页面置换算法**（先进先出页面置换）：总是淘汰最先进入内存的页面，即选择在内存中驻留时间最长的页面进行淘汰；
     5. **LRU页面置换算法**（最近最久未使用页面置换）：赋予每个页面一个访问字段，用于记录该页面上一次被访问的时间T，当淘汰一个页面时，选择现有页面的T的最大值，即最近最久未使用页面；
     6. **LFU页面置换算法**（最少使用页面置换）：该置换算法选择在之前时期使用最少的页面作为淘汰。



# 3.从BIO、NIO、AIO、Epoll、Select到计算机网络、HTTP、TCP/IP再到Netty



# 4.数据结构和算法



# 5.Java基础和集合框架



# 6.Redis+MySQL



# 7.Spring+SpringBoot+SpringCloud+Dubbo



# 8.设计模式
