# Spark

## 组成部分

* **Spark Core**：包含Spark的基本功能，尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都是构建在RDD和 Spark Core之上的；

* **Spark SQL**：提供通过Apache Hive的SQL变体Hive查询语言HiveQL与Spark进行交互的API。每个数据库表被当做一个RDD， SQL查询被转换为Spark RDD操作；

* **Spark Streaming**：对实时数据流进行处理和控制，Spark Streaming允许程序能够像普通RDD一样处理实时数据；

* **Spark Mllib**：一个常用机器学习算法库，算法被实现为对RDD的Spark操作。这个库包含可扩展的学习算法，比如分类、回归等需要对大量数据集进行迭代的操作；

* **Spark GraphX**：控制图、并行图操作和计算的一组算法和工具的集合。Spark GraphX扩展了RDD API，包含控制图、创建子图、访问路径上所有顶点的操作。



## 架构模型

* **Cluster Manager**：控制整个集群，监控Worker。在Standalone模式中即为Master主节点，在Yarn模式中为资源管理器；
* **Worker**：负责控制计算的从节点，用于启动Executor或者Driver进程；
* **Driver**：运行Spark Application的main方法；
* **Executor**：执行器，是某个Spark Application运行在Worker上的一个进程。



## 编程模型

Spark应用程序从编写、提交、执行到输出的整个过程：

* 用户使用SparkContext提供的API编写Driver Application程序。此外跟高级的SQLContext、 HiveContext及StreamingContext对 SparkContext 进行封装，并提供了SQL、 Hive及流式计算相关的API；
* 使用SparkContext提交的用户应用程序，首先会使用BlockManager和BroadcastManager将任务的Hadoop配置进行广播。然后由DAGScheduler将任务转换为RDD并组织成 DAG，DAG还将被划分为不同的Stage。最后由TaskScheduler借助ActorSystem将任务提交给集群管理器Cluster Manager；
* 集群管理器ClusterManager给任务分配资源，即将具体任务分配到Worker上。Worker创建Executor来处理任务的运行。Standalone、 YARN、 Mesos、 EC2 等都可以作为 Spark的集群管理器。 



## 计算模型

RDD弹性分布式数据集，可以看做是对各种数据计算模型的统一抽象，Spark 的计算过程主要是RDD的迭代计算过程。RDD的迭代计算过程非常类似于管道，每个RDD又存在多个分区，每个分区的数据只会在一个Task中计算，所有的分区可以在多个机器节点的 Executor上并行执行。 



## 运行流程

* 构建Spark Application的运行环境，启动SparkContext。向资源管理器申请运行Executor资源并启动StandaloneExecutorbackend；
* Executor向SparkContext申请Task；
* SparkContext将应用程序分发给Executor；
* SparkContext构建DAG图，并将DAG分解成Stage，将Task发送给Task Scheduler，最后由Task Scheduler将Task发送给 Executor运行；
* Task在Executor上运行，运行完后释放所有资源。 



## RDD模型

* 创建RDD对象；
* DAGScheduler模块进入运算，计算RDD之间的依赖关系，RDD之间的依赖关系就形成了DAG；
* 每一个Job被分为多个Stage，划分Stage的一个主要依据是RDD的宽窄依赖；
* **创建RDD**：
  * 从Hadoop文件系统（或与Hadoop兼容的其他持久化存储系统，如Hive、 Cassandra、HBase）输入数据创建；
  * 从父RDD转换得到新RDD；
  * 通过parallelize或makeRDD将单机数据创建为分布式RDD。

* **转换（Transformation）**： Transformation类操作是延迟执行的，也就是说从一个RDD转换生成另一个RDD的操作不是立即执行，而是需要有Action操作才会触发执行。 

* **行动（Action）**：Action类算子会触发Spark的作业Job提交，并将数据输出到Spark系统。