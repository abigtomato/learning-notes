# Hadoop

## HDFS存储模型

* 文件线性切割成block块，偏移量offset的单位是byte；
* block块分散存储在集群的各个节点中；
* 单一文件切分的block块大小是一致的，默认大小为128mb，可以设置block大小；
* block存在副本，副本分散在不同的节点中，默认副本数是3，可以设置副本数（注：副本数不要设置超过节点数）；
* 已上传的文件block副本数可以调整，block大小不能变化；
* hdfs只支持一次写入多次读取，同一时刻只有一个写入者；
* 文件可以append追加数据，hdfs会新增block块存储新数据，并创建副本；
* block副本放置策略：
  * 第一个副本：
    * 集群内提交：
      * 放置在上传文件的datanode中。
    * 集群外提交：
      * 随机挑选一台磁盘不太慢，cpu不太忙的节点。
  * 第二个副本：
    * 放置在与第一个副本不同机架的节点上。
  * 第三个副本：
    * 放置在与第二个副本相同机架的节点上。
  * 更多副本：
    * 随机挑选节点



## HDFS架构模型

**HDFS Client**：

* client与namenode交互元数据信息；
* client与datanode交互文件block数据。

**NameNode（NN）**：

* 保存文件的元数据（如文件大小，时间，block列表，分片位置信息，副本位置信息）；
* 基于内存存储元数据信息，不会和磁盘发生交换；
* 持久化：
  * fsimage：元数据存储到磁盘的文件名为fsimage（内存的快照），fsimage只在集群第一次启动时创建空的文件；
  * editslog：记录了对元数据的操作日志，每隔一段时间与fsimage合并（执行日志记录的操作），生成新的fsimage。

**DataNode（DN）**：

* 保存文件的block数据在磁盘上，同时存储block的元数据文件（MD5校验是否损坏）；
* datanode会向namenode上报心跳数据（3秒一次），提交block列表；
  * 如果namenode10分钟没有收到datanode的心跳，则判定次DN挂掉，从其他DN复制副本到新DN保持副本数。

**SecondaryNameNode（SNN）**：

* 帮助namenode合并fsimage和editslog（避免namenode磁盘IO消费资源）；
* SNN执行合并的时机：
  * 根据配置文件的时间间隔配置项fs.checkpoint.period，默认3600秒；
  * 根据配置文件设置editslog大小配置项fs.checkpoint.size规定，edits文件的最大默认值为64mb。



## HDFS读/写流程

**写流程**：

* client端将要写入的文件进行切分，block大小128mb；
* 与NN交互获取第一个block副本存放的DN列表；
* 将切分后的block再次切分为小文件，小文件大小为64kb；
* client根据从NN获取的DN列表，与其中DN交互，将小文件进行流式传输；
* 第一个DN接收到文件后流式传输副本到下一个DN，以pipeline的方式依次类推直到所有存放副本的DN都将副本写入完毕为止；
* block传输结束后：
  * DN向NN汇报block的信息，NN进行元数据的存储；
  * DN向client汇报写入完成；
  * client向NN汇报写入完成。
* client获取下一个block存放的DN列表，反复执行流式传输，直到文件的block全部写入完毕；
* 最终client汇报完成；
* NN会在写流程更新文件状态。

**读流程**：

* 与NN建立通信，获取一部分block副本的位置列表；
* 线性的从DN获取block，最终合并为一个文件；
* 在block副本列表中按距离择优选择DN。



## HA高可用集群

**HDFS 2.x**

* 解决单点故障：
  * HDFS HA（高可用）：通过主备NN解决，Active NN发生故障，切换到Standby NN。
* 解决内存受限：
  * HDFS Federation（联邦）：水平扩展支持多个NN，所有NN共享DN的存储资源，每个NN分管一部分的目录树结构（保存元数据）。

**HadoopHA 架构：**

* client与NN Active交互元数据，与DN交互block块，但不与NN Standby做交互；
* 所有的DN会同时向两个NN汇报block位置信息；
* NN Active会将元数据写入JournalNode集群（NN之间数据共享），JNN集群过半的节点返回成功消息则代表NN写入成功；
* NN Standby会读取JNN中的元数据，和NN Active保持数据同步；
* 两台NN节点中都存在Zookeeper Failover Controller（ZK的客户端进程）进程，ZKFC进程会与NN和ZK集群两端通信，与NN通信的进程监控NN的健康状态，这两个进程会在ZK集群的目录树结构中争抢创建文件的权利，当某个ZKFC进程成功创建文件，那这个进程管理的NN就是NN Active；
* 当NN Active挂掉，ZKFC进程接收不到心跳，会立即将ZK目录树节点上的文件删除产生事件触发回调，ZKFC Standby进程监听该事件（等待），一但发生事件，ZK将回调ZKFC Standby进程，在ZK集群中创建文件，并将NN Standby提升为NN Active，此时由此节点为client提供服务；
* 当ZKFC Active挂掉，ZK集群的session机制会启动，此时ZKFC Active与ZK集群的socket通信会断开，ZK集群会进行倒计时，计时完毕会产生事件回调，ZKFC Standby创建文件并提升NN Standby为NN Active（两个ZKFC进程还存在隐藏的与对方NN的通信，在提升自己管理的NN为主时会先尝试将对方的NN降级）。



## MapReduce原理

**MapTask**：

* Input Split：原始数据通过split逻辑进行分割；
* Map：多个map按照逻辑对分割后的所有块并行计算，结果会映射成（k，v）格式，并对处理的数据进行分区；
* Buffer In Memory：map处理后的数据会先写入内存缓冲，累加到100MB时会溢写到磁盘；
* Sort：写入磁盘会落地成小文件，小文件内部按照快速排序对相同key的数据分组；
* Merge：所有小文件会通过归并排序合并成一个文件，并行计算的map task阶段会产生多个文件，文件由reduce处理。

**ReduceTask**：

* Merge：reduce task会从多个map task拉取文件，会将一定数量的文件通过归并算法合并；
* Merge：合并后的文件会以归并算法传入reduce的逻辑进行处理；
* Reduce：会按照reduce的逻辑对数据进行处理；
* Output：将计算后的结果输出。



## Yarn资源调度集群

**Yarn集群架构**：

* yarn集群属于主从架构：
  * Resource Manager：管理集群所有的资源
  * NodeManager：管理本节点的资源，任务，并以心跳的方式向RM汇报
  * container：计算框架中的所有角色都由container表示，代表节点的资源单位；
* client提交job后，RM会挑选一台不太忙的节点启动Applocation Master管理当前job的资源调度；
* AM启动完成会回去向RM汇报，由RM决策job任务移动的目标点（container），NM默认启动线程监控container大小，一旦提交的job任务超出了申请资源的额度，会将job杀死；
* 由AM决定job任务的阶段（如MR的map和reduce阶段）提交到哪块container执行，并且job任务的执行情况还会汇报给AM；
* 如果其他的计算框架提交job，RM会在其他节点启动属于该框架的app master，框架之间的资源调度互相隔离。



## Hadoop伪分布式搭建

1. **安装JDK/Hadoop**

```bash
$> mkdir /opt/sxt
$> cd ~
$> rpm -i ./jdk-7u67-linux-x64.rpm
$> tar -zxvf ./hadoop-2.6.5.tar.gz -C /opt/sxt
$> cd /usr/java/jdk1.7.0_67
```

2. **配置环境变量**

```bash
$> vim /etc/profile # 设置环境变量
    export JAVA_HOME=/usr/java/jdk1.7.0_67
    export HADOOP_PREFIX=/opt/sxt/hadoop-2.6.5
    export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin
$> source ~/.bashrc # 使环境变量立即生效
```

3. **配置SSH免密登录**

```bash
$> ssh localhost # 远程登陆本机——可以
$> exit # 退出ssh登陆
$> ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa # 生成密钥
$> cat ~/.ssh/id_dsa.put >> ~/.ssh/authorized_keys # 添加授权
```

4. **修改配置文件**

```bash
$> cd /opt/sxt/hadoop-2.6.5/etc/hadoop
$> vim hadoop-env.sh
    export JAVA_HOME=/usr/java/jdk1.7.0_67
$> vim mapred-env.sh
    export JAVA_HOME=/usr/java/jdk1.7.0_67
$> vim yarn-env.sh
    export JAVA_HOME=/usr/java/jdk1.7.0_67
$> vim core-site.xml
    <property>
        <name>fs.defaultFS</name> # 设置namenode启动节点
        <value>hdfs://node01:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name> # 设置hadoop临时文件的存储目录
        <value>/var/sxt/hadoop/local</value>
    </property>
$> vim hdfs-site.xml
    <property>
        <name>dfs.replication</name> # 设置hdfs文件的副本数量
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node01:50090</value>
    </property>
$> vim slaves # 设置datanode启动节点
    Node01
```

5. **启动Hadoop**

```bash
$> hdfs namenode -format # 格式化namenode（生成VERSION文件，datanode会拷贝VERSION以判断哪些节点组成一个集群）
$> start-dfs.sh # 启动hadoop
$> jps # 查看后台java进程
$> cat /var/sxt/hadoop/local/dfs/data/current/VERSION # 查看VERSION文件中的集群ID是否和启动时显示的ID一致
$> stop-dfs.sh # 关闭hadoop
```



## Hadoop集群搭建

1. **时间同步和剩余节点安装JDK**

```bash
node01, node02, node03, node04：
    $> date -s "2018-03-12 14:31:00"
node02,node03,node04：
    $> rpm -i ~/jdk-7u67-linux-x64.rpm
```

2. **集群的ssh免密配置**

```bash
node01：
    $> cd ~/.ssh
    $> scp ~/.ssh/id_dsa.pub root@node02:`pwd`/node01.pub
    $> scp ~/.ssh/id_dsa.pub root@node03:`pwd`/node01.pub
    $> scp ~/.ssh/id_dsa.pub root@node04:`pwd`/node01.pub
node02, node03, node04：
    $> ssh localhost
    $> cat ~/.ssh/node01.pub >> ~/.ssh/authorized_keys
```

3. **同步环境变量到集群**

```bash
node01：
    $> scp /etc/profile node02:/etc/profile
    $> scp /etc/profile node03:/etc/profile
    $> scp /etc/profile node04:/etc/profile
node02, node03, node04：
    $> source /etc/profile
```

4. **修改集群配置文件**

```bash
node01：
    $> cd /opt/sxt/hadoop/etc
    $> cp -r hadoop hadoop-local
    $> cd hadoop
    $> vim core-site.xml
        <property>
            <name>fs.defaultFS</name> 
            <value>hdfs://node01:9000</value>
        </property>
        <property>
            <name>hadoop.tmp.dir</name> 
            <value>/var/sxt/hadoop/full</value>
        </property>
    $> vim slaves
        node02
        node03
        node04
    $> vim hdfs-site.xml
        <property>
            <name>dfs.replication</name>
            <value>2</value>
        </property>
        <property>
            <name>dfs.namenode.secondary.http-address</name>
            <value>node02:50090</value>
        </property>
```

5. **同步hadoop到集群所有节点**

```bash
node01：
    $> cd /opt
    $> scp -r sxt node02:`pwd` # pwd表示当前目录
    $> scp -r sxt node03:`pwd`
    $> scp -r sxt node04:`pwd`
```

6. **启动hadoop集群**

```bash
node01：
    $> hdfs namenode -format
    $> start-dfs.sh
```



## Hadoop HA高可用集群搭建

1. **NN Standby与NN Active与免密钥操作：**

```bash
node02（NN Standby）：
$> cd ~/.ssh
$> ssh-keygen -t dsa -P '' -f ./id_dsa
$> cat id_dsa.pub >> authorized_keys
$> scp id_dsa.pub node01:`pwd`/node02.pub
node01（NN Active）：
$> cat ~/.ssh/node02.pub >> authorized_keys
```

2. **配置hdfs-site.xml文件：**

```bash
node01：
$> cd /opt/sxt/hadoop-2.6.5/etc/
$> cp -r hadoop hadoop-full
$> cd hadoop
$> vim hdfs-site.xml
    <property>
        # 配置Block副本数
        <name>dfs.replication</name> 
        <value>3</value>
    </property>
    <property>
        # 配置HA服务名称
        <name>dfs.nameservices</name> 
        <value>mycluster</value>
    </property>
    <property>
        # 配置服务名称的映射
        <name>dfs.ha.namenodes.mycluster</name> 
        <value>nn1,nn2</value>
    </property>
    <property>
        # 配置nn1映射的物理机
        <name>dfs.namenode.rpc-address.mycluster.nn1</name> 
        # 8020是rpc-address进程间通信的端口
        <value>node01:8020</value>
    </property>
    <property>
        # 配置nn2映射的物理机
        <name>dfs.namenode.rpc-address.mycluster.nn2</name> 
        <value>node02:8020</value>
    </property>
    <property>
        # 50070是http协议端口
        <name>dfs.namenode.http-address.mycluster.nn1</name>
        <value>node01:50070</value> 
    </property>
    <property>
        <name>dfs.namenode.http-address.mycluster.nn2</name>
        <value>node02:50070</value>
    </property>
    <property>
        # 配置journalnode集群所有节点的物理机，/mycluster表示ha服务的目录
        <name>dfs.namenode.shared.edits.dir</name>
        # journalnode集群可以为多个ha集群提供服务
        <value>qjournal://node01:8485;node02:8485;node03:8485/mycluster</value> 
    </property>
    <property>
        # 配置JNN集群存储edits元数据文件的目录
        <name>dfs.journalnode.edits.dir</name> 
        <value>/var/sxt/hadoop/ha/jn</value>
    </property>
    # 此部分是故障转移实现的方法（通过ssh免密钥）
    <property>
        <name>dfs.client.failover.proxy.provider.mycluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
</property>
<property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/root/.ssh/id_dsa</value>
</property>
<property>
    # 配置集群会启动ZKFC进程
    <name>dfs.ha.automatic-failover.enabled</name> 
    <value>true</value>
</property>
```

3. **配置core-site.xml文件：**

```bash
node01：
    $> vim core-site.xml
        <property>
            # 配置NN的入口（填写HA集群的逻辑映射）
            <name>fs.defaultFS</name> 
            <value>hdfs://mycluster</value>
        </property>
        <property>
            # 配置hadoop临时文件目录
            <name>hadoop.tmp.dir</name> 
            <value>/var/sxt/hadoop/ha</value>
        </property>
        <property>
            # 使ZKFC进程可以和ZK集群通信
            <name>ha.zookeeper.quorum</name> 
            <value>node02:2181,node03:2181,node04:2181</value>
        </property>
```

4. **分发配置文件到集群：**

```bash
node01：
    $> cd /opt/sxt/hadoop-2.6.5/etc/hadoop
    $> scp hdfs-site.xml core-siter.xml node02:`pwd`
    $> scp hdfs-site.xml core-siter.xml node03:`pwd`
    $> scp hdfs-site.xml core-siter.xml node04:`pwd`
```

5. **搭建Zookeeper集群：**

```bash
node02：
    $> tar -zxvf ~/zookeeper-3.4.6.tar.gz -C /opt/sxt/
    $> vim /etc/profile # 配置ZK的环境变量
        export ZOOKEEPER_PREFIX=/opt/sxt/zookeeper-3.4.6
        export PATH=$PATH:$HADOOP_HOME/bin:$ZOOKEEPER_PREFIX/bin
    $> source /etc/profile
    $> cd /opt/sxt/zookeeper-3.4.6/conf
    $> cp zoo_sample.cfg zoo.cfg
    $> vim ./zoo.cfg
        dataDir=/var/sxt/zk # 配置ZK的目录树数据的存储目录
        server.1=192.168.12.102:2888:3888 # 2888表示在主从架构下集群间传递数据的端口
        server.2=192.168.12.103:2888:3888 # 3888表示无主情况下集群间通过投票选举主节点所用的端口
        server.3=192.168.12.104:2888:3888
    $> mkdir -p /var/sxt/zk
    $> vi /var/sxt/zk/myid
    $> echo 1 > /var/sxt/zk/myid # 设置集群的唯一id
    $> cd /opt/sxt/
    $> scp -r zookeeper-3.4.6 node03:`pwd` # 同步ZK到集群
    $> scp -r zookeeper-3.4.6 node04:`pwd`
    $> scp /etc/profile node03:/etc/profile # 同步环境变量到集群
    $> scp /etc/profile node04:/etc/profile
node03：
    $> mkdir -p /var/sxt/zk
    $> echo 2 > /var/sxt/zk/myid
    $> source /etc/profile
node04：
    $> mkdir -p /var/sxt/zk
    $> echo 3 > /var/sxt/zk/myid
    $> source /etc/profile
node02, node03, node04：
    $> zkServer.sh start
```

6. **启动JournalNode集群：**

```bash
node01, node02, node03：
    $> hadoop-daemon.sh start journalnode
```

7. **启动主备NameNode：**

```bash
node01：
    $> hdfs namenode -format # 第一台NN格式化namnode进程，生成fsimage和集群id
    $> hadoop-daemon.sh start namenode
node02：
    $> hdfs namenode -bootstrapStandby # 第二台NN执行脚本拷贝第一台NN产生的文件
```

8. **启动ZKFC进程：**

```bash
node01：
    $> hdfs zkfc -formatZK # 格式化ZKFC进程（ZK是一个公共的集群，有很多集群可以使用，为了不发生冲突，在ZK的目录树中创建属于本集群的目录）
    $> start-dfs.sh
```

9. **非第一次启动Hadoop HA集群：**

```bash
node02, node03, node04：
    $> zkServer.sh start # 启动Zookeeper集群
node01：
    $> start-dfs.sh # 启动高可用集群的其他成员
```



## Yarn资源调度集群搭建

1. **配置mapred-site.xml文件**

```bash
node01：
$> cd /opt/sxt/hadoop-2.6.5/etc/hadoop
$> cp mapred-site.xml.template mapred-site.xml
$> vim mapred-site.xml
    <property>
        # 启动MapReduce On Yarn模式
        <name>mapreduce.framework.name</name> 
        <value>yarn</value>
    </property>
```

2. **配置yarn-site.xml文件**

```bash
node01：
    $> vim yarn-site.xml
        <property>
            # 配置yarn作用到shuffle阶段
            <name>yarn.nodemanager.aux-services</name> 
            <value>mapreduce_shuffle</value>
        </property>
        <property>
            # 配置yarn启动HA模式
            <name>yarn.resourcemanager.ha.enabled</name> 
            <value>true</value>
        </property>
        <property>
            # 配置集群的id
            <name>yarn.resourcemanager.cluster-id</name> 
            <value>cluster1</value>
        </property>
        <property>
            # 配置ha模式下两个RM的逻辑名称
            <name>yarn.resourcemanager.ha.rm-ids</name> 
            <value>rm1,rm2</value>
        </property>
        <property>
            # 配置名称rm1映射的物理主机
            <name>yarn.resourcemanager.hostname.rm1</name> 
            <value>node03</value>
        </property>
        <property>
            # 配置名称rm2映射的物理主机
            <name>yarn.resourcemanager.hostname.rm2</name> 
            <value>node04</value>
        </property>
        <property>
            # 配置RM与zookeeper集群通信
            <name>yarn.resourcemanager.zk-address</name> 
            <value>node02:2181,node03:2181,node04:2181</value>
        </property>
```

3. **同步配置文件到集群**

```bash
node01：
    $> scp mapred-site.xml yarn-site.xml node02:`pwd`
    $> scp mapred-site.xml yarn-site.xml node03:`pwd`
    $> scp mapred-site.xml yarn-site.xml node04:`pwd`
```

4. **两个RM进程物理机的相互免密钥**

```bash
node03：
    $> cd ~/.ssh
    $> ssh-keygen -t dsa -P '' -f ./id_dsa
    $> cat id_dsa.pub >> authorized_keys
    $> scp id_dsa.pub root@node04:`pwd`/node03.pub
node04：
    $> cd ~/.ssh
    $> cat node03.pub >> authorized_keys
    $> ssh-keygen -t dsa -P '' -f ./id_dsa
    $> cat id_dsa.pub >> authorized_keys
    $> scp id_dsa.pub root@node03:`pwd`/node04.pub
node03：
    $> cat node04.pub >> authorized_keys
```

5. **启动yarn集群**

```bash
node02, node03, node04：
    $> zkServer.sh start
node01：
    $> start-dfs.sh
    $> start-yarn.sh
node03，node04：
    $> yarn-daemon.sh start resourcemanager
```

6. **在搭建完备（hdfs-ha和yarn-ha）的集群上运行MapReduce示例程序**

```bash
node1：
    $> cd /opt/sxt/hadoop-2.6.5/share/hadoop/mapreduce
    $> hadoop jar hadoop-mapreduce-examples-2.6.5.jar wordcount /user/root /wordcount
    $> hdfs dfs -cat /wordcount/part-r-00000
```